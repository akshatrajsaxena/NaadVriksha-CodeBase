{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98a152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def preprocessData(input_file, output_file):\n",
    "    df = pd.read_csv(input_file)\n",
    "    print(f\"Original data shape: {df.shape}\")\n",
    "    print(\"Original columns:\", df.columns.tolist())\n",
    "    print(\"\\nFirst few rows before preprocessing:\")\n",
    "    print(df.head())\n",
    "    def methods(value):\n",
    "        if pd.isna(value):\n",
    "            return np.nan\n",
    "\n",
    "        value_str = str(value)\n",
    "        if ':' in value_str:\n",
    "            value_str = value_str.split(':', 1)[1]\n",
    "        value_str = value_str.strip().rstrip(',')\n",
    "\n",
    "        if re.match(r'^\\d+\\.$', value_str):\n",
    "            value_str = value_str + '00'\n",
    "\n",
    "        return value_str\n",
    "\n",
    "    colClean = ['Temperature', 'Humidity', 'Acc_X', 'Acc_Y', 'Acc_Z','Gyro_X', 'Gyro_Y', 'Gyro_Z', 'LDR']\n",
    "    for i in colClean:\n",
    "        if i in df.columns:\n",
    "            df[i] = df[i].apply(methods)\n",
    "\n",
    "    if 'Timestamp' in df.columns:\n",
    "        df['Timestamp'] = df['Timestamp'].apply(lambda x: x.split(' ')[1] if ' ' in str(x) else x)\n",
    "\n",
    "    print(\"\\nAfter cleaning prefixes and formatting:\")\n",
    "    print(df.head())\n",
    "\n",
    "    numeric_columns = ['Temperature', 'Humidity', 'Acc_X', 'Acc_Y', 'Acc_Z','Gyro_X', 'Gyro_Y', 'Gyro_Z', 'LDR']\n",
    "    for i in numeric_columns:\n",
    "        if i in df.columns:\n",
    "            df[i] = pd.to_numeric(df[i], errors='coerce')\n",
    "\n",
    "    print(f\"\\nData shape before dropna: {df.shape}\")\n",
    "    print(\"Missing values per column:\")\n",
    "    print(df.isnull().sum())\n",
    "    missing = df.isnull().any(axis=1)\n",
    "    for i in numeric_columns:\n",
    "        if i in df.columns:\n",
    "            empty_mask = (df[i] == '') | (df[i] == ' ')\n",
    "            missing = missing | empty_mask\n",
    "\n",
    "    df_cleaned = df[~missing].copy()\n",
    "\n",
    "    print(f\"\\nData shape after dropna: {df_cleaned.shape}\")\n",
    "    print(f\"Removed {df.shape[0] - df_cleaned.shape[0]} rows with missing values\")\n",
    "\n",
    "    for i in numeric_columns:\n",
    "        if i in df_cleaned.columns:\n",
    "            df_cleaned[i] = df_cleaned[i].apply(\n",
    "                lambda x: f\"{x:.2f}\" if pd.notna(x) else x)\n",
    "\n",
    "    print(\"\\nFinal processed data:\")\n",
    "    print(df_cleaned.head(10))\n",
    "    print(f\"\\nFinal data shape: {df_cleaned.shape}\")\n",
    "    df_cleaned.to_csv(output_file, index=False)\n",
    "    print(f\"\\nProcessed data saved to: {output_file}\")\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"./data/NaadVriksha (Responses) - Rainy.csv\"\n",
    "    output_file = \"./preprocessed_data/NaadVriksha_Rainy_Processed.csv\"\n",
    "    try:\n",
    "        processed_df = preprocessData(input_file, output_file)\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"PREPROCESSING COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Input file: {input_file}\")\n",
    "        print(f\"Output file: {output_file}\")\n",
    "        print(f\"Rows processed: {len(processed_df)}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find the file '{input_file}'\")\n",
    "        print(\"Please make sure the file exists in the current directory.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a733173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "file_paths = [\"./preprocessed_data/NaadVriksha_Cloudy_Processed.csv\", \"./preprocessed_data/NaadVriksha_Rainy_Processed.csv\", \"./preprocessed_data/NaadVriksha_Stormy_Processed.csv\", \"./preprocessed_data/NaadVriksha_Sunny_Processed.csv\", \"./preprocessed_data/NaadVriksha_Windy_Processed.csv\"]\n",
    "dfs = [pd.read_csv(file) for file in file_paths]\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "numeric_cols = ['Temperature', 'Humidity', 'Acc_X', 'Acc_Y', 'Acc_Z','Gyro_X', 'Gyro_Y', 'Gyro_Z', 'LDR']\n",
    "scaler = MinMaxScaler()\n",
    "combined_df[numeric_cols] = scaler.fit_transform(combined_df[numeric_cols])\n",
    "final_output_path = \"./finalData.csv\"\n",
    "combined_df.to_csv(final_output_path, index=False)\n",
    "final_output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02e7d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "\n",
    "def directoryStruct(base_dir):\n",
    "    weather_conditions = ['Sunny', 'Rainy', 'Cloudy', 'Stormy', 'Windy']\n",
    "    splits = ['train', 'val', 'test']\n",
    "    for weather in weather_conditions:\n",
    "        for split in splits:\n",
    "            dir_path = os.path.join(base_dir, f\"{weather}Split\", split)\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "            print(f\"Created directory: {dir_path}\")\n",
    "    \n",
    "    \n",
    "    combined_dir = os.path.join(base_dir, 'Combined')\n",
    "    os.makedirs(combined_dir, exist_ok=True)\n",
    "    print(f\"Created directory: {combined_dir}\")\n",
    "\n",
    "def splitData(file_path, weather_name, output_base_dir, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"\\nProcessing {weather_name} data...\")\n",
    "    print(f\"Original shape: {df.shape}\")\n",
    "    \n",
    "    total_samples = len(df)\n",
    "    train_size = int(total_samples * train_ratio)\n",
    "    val_size = int(total_samples * val_ratio)\n",
    "    test_size = total_samples - train_size - val_size  \n",
    "    train_data = df.iloc[:train_size].copy()\n",
    "    val_data = df.iloc[train_size:train_size + val_size].copy()\n",
    "    test_data = df.iloc[train_size + val_size:].copy()    \n",
    "    print(f\"Split sizes - Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n",
    "    weather_dir = f\"{weather_name}Split\"\n",
    "    \n",
    "    \n",
    "    train_path = os.path.join(output_base_dir, weather_dir, 'train', f'{weather_name}_train.csv')\n",
    "    train_data.to_csv(train_path, index=False)\n",
    "    print(f\"Saved train data to: {train_path}\")\n",
    "    val_path = os.path.join(output_base_dir, weather_dir, 'val', f'{weather_name}_val.csv')\n",
    "    val_data.to_csv(val_path, index=False)\n",
    "    print(f\"Saved validation data to: {val_path}\")\n",
    "    test_path = os.path.join(output_base_dir, weather_dir, 'test', f'{weather_name}_test.csv')\n",
    "    test_data.to_csv(test_path, index=False)\n",
    "    print(f\"Saved test data to: {test_path}\")\n",
    "    \n",
    "    return {'train': train_data,'val': val_data,'test': test_data}\n",
    "\n",
    "def create_combined_datasets_with_normalization(output_base_dir):\n",
    "    weather_conditions = ['Sunny', 'Rainy', 'Cloudy', 'Stormy', 'Windy']\n",
    "    splits = ['train', 'val', 'test']\n",
    "    numerical_cols = ['Temperature', 'Humidity', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyro_X', 'Gyro_Y', 'Gyro_Z', 'LDR']\n",
    "    combined_dir = os.path.join(output_base_dir, 'Combined')\n",
    "    combined_datasets = {}\n",
    "    \n",
    "    for split in splits:\n",
    "        combined_data = []\n",
    "        print(f\"\\nCombining {split} data from all weather conditions...\")\n",
    "        for weather in weather_conditions:\n",
    "            file_path = os.path.join(output_base_dir, f\"{weather}Split\", split, f'{weather}_{split}.csv')\n",
    "            \n",
    "            if os.path.exists(file_path):\n",
    "                df = pd.read_csv(file_path)\n",
    "                combined_data.append(df)\n",
    "                print(f\"Added {weather} {split} data: {len(df)} samples\")\n",
    "            else:\n",
    "                print(f\"Warning: {file_path} not found\")\n",
    "        \n",
    "        if combined_data:\n",
    "            combined_df = pd.concat(combined_data, ignore_index=True)\n",
    "            combined_datasets[split] = combined_df\n",
    "            print(f\"Combined {split} dataset: {len(combined_df)} samples\")\n",
    "            print(f\"Weather distribution in {split}:\")\n",
    "            print(combined_df['Weather'].value_counts())\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"APPLYING NORMALIZATION TO COMBINED DATASETS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    if 'train' in combined_datasets:\n",
    "        train_df = combined_datasets['train']\n",
    "        existing_numerical_cols = [col for col in numerical_cols if col in train_df.columns]\n",
    "        print(f\"Normalizing columns: {existing_numerical_cols}\")\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(train_df[existing_numerical_cols])\n",
    "        for split, df in combined_datasets.items():\n",
    "            print(f\"\\nNormalizing {split} data...\")\n",
    "            normalized_df = df.copy()\n",
    "            normalized_df[existing_numerical_cols] = scaler.transform(df[existing_numerical_cols])\n",
    "            print(f\"Data ranges after normalization in {split}:\")\n",
    "            for col in existing_numerical_cols:\n",
    "                print(f\"  {col}: {normalized_df[col].min():.3f} to {normalized_df[col].max():.3f}\")            \n",
    "            normalized_path = os.path.join(combined_dir, f'combined_{split}_normalized.csv')\n",
    "            normalized_df.to_csv(normalized_path, index=False)\n",
    "            print(f\"Saved normalized {split} data to: {normalized_path}\")\n",
    "            non_normalized_path = os.path.join(combined_dir, f'combined_{split}_raw.csv')\n",
    "            df.to_csv(non_normalized_path, index=False)\n",
    "            print(f\"Saved raw {split} data to: {non_normalized_path}\")\n",
    "    \n",
    "    return combined_datasets\n",
    "\n",
    "def main():\n",
    "    preprocessed_data_dir = 'preprocessed_data'\n",
    "    output_base_dir = 'split_data'\n",
    "    files_to_process = {'Sunny': 'NaadVriksha_Sunny_Processed.csv','Rainy': 'NaadVriksha_Rainy_Processed.csv','Cloudy': 'NaadVriksha_Cloudy_Processed.csv','Stormy': 'NaadVriksha_Stormy_Processed.csv','Windy': 'NaadVriksha_Windy_Processed.csv'}\n",
    "    \n",
    "    print(\"Creating directory structure...\")\n",
    "    directoryStruct(output_base_dir)\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"STEP 1: SPLITTING DATA WITHOUT NORMALIZATION\")\n",
    "    print(f\"{'='*50}\")\n",
    "    all_splits = {}\n",
    "    for weather_name, filename in files_to_process.items():\n",
    "        file_path = os.path.join(preprocessed_data_dir, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            splits = splitData(file_path=file_path,weather_name=weather_name,output_base_dir=output_base_dir,train_ratio=0.7,val_ratio=0.15,test_ratio=0.15)\n",
    "            all_splits[weather_name] = splits\n",
    "        else:\n",
    "            print(f\"Warning: File {file_path} not found!\")\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"STEP 2: COMBINING AND NORMALIZING DATA\")\n",
    "    print(f\"{'='*50}\")\n",
    "    combined_datasets = create_combined_datasets_with_normalization(output_base_dir)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"SUMMARY\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    print(\"Individual Weather Splits (Raw Data):\")\n",
    "    for weather_name, splits in all_splits.items():\n",
    "        print(f\"{weather_name}:\")\n",
    "        print(f\"  Train: {len(splits['train'])} samples\")\n",
    "        print(f\"  Val: {len(splits['val'])} samples\")\n",
    "        print(f\"  Test: {len(splits['test'])} samples\")\n",
    "        print(f\"  Total: {len(splits['train']) + len(splits['val']) + len(splits['test'])} samples\")\n",
    "    \n",
    "    print(f\"\\nCombined Datasets:\")\n",
    "    for split, df in combined_datasets.items():\n",
    "        print(f\"Combined {split}: {len(df)} samples\")\n",
    "        print(f\"  Weather distribution: {dict(df['Weather'].value_counts())}\")\n",
    "    \n",
    "    print(f\"\\nAll data saved in: {output_base_dir}/\")\n",
    "    print(\"Final Directory Structure:\")\n",
    "    print(\"├─ split_data/\")\n",
    "    print(\"│  ├─ SunnySplit/\")\n",
    "    print(\"│  │  ├─ train/ (Sunny_train.csv)\")\n",
    "    print(\"│  │  ├─ val/ (Sunny_val.csv)\")\n",
    "    print(\"│  │  └─ test/ (Sunny_test.csv)\")\n",
    "    print(\"│  ├─ RainySplit/ ...\")\n",
    "    print(\"│  ├─ CloudySplit/ ...\")\n",
    "    print(\"│  ├─ StormySplit/ ...\")\n",
    "    print(\"│  ├─ WindySplit/ ...\")\n",
    "    print(\"│  └─ Combined/\")\n",
    "    print(\"│     ├─ combined_train_raw.csv\")\n",
    "    print(\"│     ├─ combined_train_normalized.csv\")\n",
    "    print(\"│     ├─ combined_val_raw.csv\")\n",
    "    print(\"│     ├─ combined_val_normalized.csv\")\n",
    "    print(\"│     ├─ combined_test_raw.csv\")\n",
    "    print(\"│     └─ combined_test_normalized.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3e1bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "weather_encoding = {\"Sunny\": 0,\"Rainy\": 1,\"Stormy\": 2,\"Windy\": 3,\"Cloudy\": 4}\n",
    "folder_path = \"split_data/Combined\"\n",
    "files = {\"train\": \"combined_train_normalized.csv\",\"val\": \"combined_val_normalized.csv\",\"test\": \"combined_test_normalized.csv\"}\n",
    "\n",
    "for split, filename in files.items():\n",
    "    file_path = f\"{folder_path}/{filename}\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"Weather\"] = df[\"Weather\"].map(weather_encoding)\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"{split.capitalize()} file encoded and saved: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f07710",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEncoding 'Weather' column in combined datasets...\")\n",
    "print(\"The weathers are encoded as follows:\")\n",
    "for weather, code in weather_encoding.items():\n",
    "    print(f\"{weather}: {code}\")\n",
    "print(\"\\nEncoding 'Weather' column in combined datasets...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
